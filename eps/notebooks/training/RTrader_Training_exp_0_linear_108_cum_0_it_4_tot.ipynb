{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BID1eylMmBy",
        "outputId": "ad4bd650-5459-4d9f-9c4c-eeefe860a534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'r_trader'...\n",
            "remote: Enumerating objects: 12921, done.\u001b[K\n",
            "remote: Counting objects: 100% (3835/3835), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1069/1069), done.\u001b[K\n",
            "remote: Total 12921 (delta 2826), reused 3759 (delta 2750), pack-reused 9086 (from 1)\u001b[K\n",
            "Receiving objects: 100% (12921/12921), 76.52 MiB | 22.00 MiB/s, done.\n",
            "Resolving deltas: 100% (9158/9158), done.\n",
            "Branch 'deep-reinforcement.training-experiment-linear' set up to track remote branch 'deep-reinforcement.training-experiment-linear' from 'origin'.\n",
            "Switched to a new branch 'deep-reinforcement.training-experiment-linear'\n",
            "Collecting cattrs\n",
            "  Downloading cattrs-24.1.2-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting positional-encodings==6.0.1\n",
            "  Downloading positional_encodings-6.0.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting dropbox\n",
            "  Downloading dropbox-12.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pymongo==4.3.3\n",
            "  Downloading pymongo-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.6 kB)\n",
            "Collecting dependency-injector==4.41.0\n",
            "  Downloading dependency_injector-4.41.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from positional-encodings==6.0.1) (1.26.4)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo==4.3.3)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: six<=1.16.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from dependency-injector==4.41.0) (1.16.0)\n",
            "Requirement already satisfied: attrs>=23.1.0 in /usr/local/lib/python3.10/dist-packages (from cattrs) (24.2.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from cattrs) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions!=4.6.3,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from cattrs) (4.12.2)\n",
            "Requirement already satisfied: requests>=2.16.2 in /usr/local/lib/python3.10/dist-packages (from dropbox) (2.32.3)\n",
            "Collecting stone<3.3.3,>=2 (from dropbox)\n",
            "  Downloading stone-3.3.1-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.16.2->dropbox) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.16.2->dropbox) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.16.2->dropbox) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.16.2->dropbox) (2024.8.30)\n",
            "Collecting ply>=3.4 (from stone<3.3.3,>=2->dropbox)\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl.metadata (844 bytes)\n",
            "Downloading positional_encodings-6.0.1-py3-none-any.whl (7.5 kB)\n",
            "Downloading pymongo-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dependency_injector-4.41.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cattrs-24.1.2-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dropbox-12.0.2-py3-none-any.whl (572 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.1/572.1 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stone-3.3.1-py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.3/162.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ply, stone, positional-encodings, dnspython, dependency-injector, cattrs, pymongo, dropbox\n",
            "Successfully installed cattrs-24.1.2 dependency-injector-4.41.0 dnspython-2.7.0 dropbox-12.0.2 ply-3.11 positional-encodings-6.0.1 pymongo-4.3.3 stone-3.3.1\n"
          ]
        }
      ],
      "source": [
        "!rm -fr r_trader out input\n",
        "!mkdir out input\n",
        "!git clone https://github.com/abreham-atlaw/r_trader\n",
        "!cd r_trader &&  git checkout deep-reinforcement.training-experiment-linear\n",
        "!pip install cattrs positional-encodings==6.0.1 dropbox pymongo==4.3.3 dependency-injector==4.41.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NglEZi_MbY1K"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/r_trader\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ywjn_CRpbb5g"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam, SGD, Adagrad\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import signal\n",
        "\n",
        "from core.utils.research.data.load.dataset import BaseDataset\n",
        "from core.utils.research.training.trainer import Trainer\n",
        "from core.utils.research.model.model.cnn.model import CNN\n",
        "from core.utils.research.model.model.linear.model import LinearModel\n",
        "from lib.utils.torch_utils.model_handler import ModelHandler\n",
        "from core.utils.research.training.callbacks.checkpoint_callback import CheckpointCallback, StoreCheckpointCallback\n",
        "from core.utils.research.training.data.repositories.checkpoint_repository import CheckpointRepository\n",
        "from lib.utils.file_storage import PCloudClient\n",
        "from core.utils.research.training.data.state import TrainingState\n",
        "from core import Config\n",
        "from core.utils.research.training.callbacks.metric_callback import MetricCallback\n",
        "from core.utils.research.training.data.repositories.metric_repository import MetricRepository, MongoDBMetricRepository\n",
        "from core.utils.kaggle import FusedManager\n",
        "from core.di import init_di, ApplicationContainer\n",
        "from core.utils.research.training.data.metric import MetricsContainer\n",
        "from core.utils.research.model.layers import Indicators\n",
        "from core.di import ServiceProvider\n",
        "from core.utils.kaggle.data_repository import KaggleDataRepository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7J5LSb9gRUR"
      },
      "outputs": [],
      "source": [
        "def download_data(root, datasets, zip_filename, kernel_mode=True):\n",
        "    repository = KaggleDataRepository(\n",
        "        output_path=root,\n",
        "        zip_filename=zip_filename\n",
        "    )\n",
        "    repository.download_multiple(datasets, kernel=kernel_mode)\n",
        "    for dataset in datasets:\n",
        "        os.system\n",
        "    os.system(f\"unzip -d root/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOhI098Nn_Qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d96d67fd-8e49-4196-fbaf-4510d317f35b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94m PID:272 [2024-10-30 17:06:00.874356]  Downloading abrehamatlaw0/spinoza-ds-datapreparer-simsim-cum-0-it-2-0 \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:06:00.876946]  Downloading to /content/input/abrehamatlaw0-spinoza-ds-datapreparer-simsim-cum-0-it-2-0 \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:06:00.878877]  Checking pre-downloaded for /content/input/abrehamatlaw0-spinoza-ds-datapreparer-simsim-cum-0-it-2-0 \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:06:00.883383]  Cleaning /content/input/abrehamatlaw0-spinoza-ds-datapreparer-simsim-cum-0-it-2-0 \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:06:01.275322]  Using Account: bemnetatlaw \u001b[0m\n",
            "Dataset URL: https://www.kaggle.com/datasets/abrehamatlaw0/spinoza-ds-datapreparer-simsim-cum-0-it-2-0\n",
            "\u001b[94m PID:272 [2024-10-30 17:06:03.460755]  Unzipping Data... \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:07:21.809515]  Downloaded False to /content/input/abrehamatlaw0-spinoza-ds-datapreparer-simsim-cum-0-it-2-0 \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:07:21.809739]  Generating checksum for '/content/input/abrehamatlaw0-spinoza-ds-datapreparer-simsim-cum-0-it-2-0' \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:07:21.859326]  Checksum: e5b36d61f56b29147d149ce0828c9964ad01a46c6e936506eef1fcf0eeb85f81 \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:07:21.859475]  Downloading abrehamatlaw0/spinoza-ds-datapreparer-simsim-cum-0-it-2-1 \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:07:21.859565]  Downloading to /content/input/abrehamatlaw0-spinoza-ds-datapreparer-simsim-cum-0-it-2-1 \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:07:21.859644]  Checking pre-downloaded for /content/input/abrehamatlaw0-spinoza-ds-datapreparer-simsim-cum-0-it-2-1 \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:07:21.865079]  Cleaning /content/input/abrehamatlaw0-spinoza-ds-datapreparer-simsim-cum-0-it-2-1 \u001b[0m\n",
            "Dataset URL: https://www.kaggle.com/datasets/abrehamatlaw0/spinoza-ds-datapreparer-simsim-cum-0-it-2-1\n",
            "\u001b[94m PID:272 [2024-10-30 17:07:24.443853]  Unzipping Data... \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:08:26.060179]  Downloaded False to /content/input/abrehamatlaw0-spinoza-ds-datapreparer-simsim-cum-0-it-2-1 \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:08:26.060394]  Generating checksum for '/content/input/abrehamatlaw0-spinoza-ds-datapreparer-simsim-cum-0-it-2-1' \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:08:26.107823]  Checksum: dc5d7c2c08ad19338f975eaa80cea4547bdef49feccd1671d33588d281eb29a4 \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:08:26.108688]  Downloading abrehamatlaw0/spinoza-ds-datapreparer-simsim-cum-0-it-2-2 \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:08:26.108788]  Downloading to /content/input/abrehamatlaw0-spinoza-ds-datapreparer-simsim-cum-0-it-2-2 \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:08:26.108907]  Checking pre-downloaded for /content/input/abrehamatlaw0-spinoza-ds-datapreparer-simsim-cum-0-it-2-2 \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:08:26.114277]  Cleaning /content/input/abrehamatlaw0-spinoza-ds-datapreparer-simsim-cum-0-it-2-2 \u001b[0m\n",
            "Dataset URL: https://www.kaggle.com/datasets/abrehamatlaw0/spinoza-ds-datapreparer-simsim-cum-0-it-2-2\n",
            "\u001b[94m PID:272 [2024-10-30 17:08:30.621471]  Unzipping Data... \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:09:40.994381]  Downloaded False to /content/input/abrehamatlaw0-spinoza-ds-datapreparer-simsim-cum-0-it-2-2 \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:09:40.994626]  Generating checksum for '/content/input/abrehamatlaw0-spinoza-ds-datapreparer-simsim-cum-0-it-2-2' \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:09:41.064634]  Checksum: d5a293c8f7e9e0512a12af79235a20b4510e6620d8d9c0b632359c44e7029185 \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:09:41.064860]  Downloading abrehamatlaw0/spinoza-ds-datapreparer-simsim-cum-0-it-2-3 \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:09:41.066670]  Downloading to /content/input/abrehamatlaw0-spinoza-ds-datapreparer-simsim-cum-0-it-2-3 \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:09:41.066744]  Checking pre-downloaded for /content/input/abrehamatlaw0-spinoza-ds-datapreparer-simsim-cum-0-it-2-3 \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:09:41.066856]  Cleaning /content/input/abrehamatlaw0-spinoza-ds-datapreparer-simsim-cum-0-it-2-3 \u001b[0m\n",
            "Dataset URL: https://www.kaggle.com/datasets/abrehamatlaw0/spinoza-ds-datapreparer-simsim-cum-0-it-2-3\n",
            "\u001b[94m PID:272 [2024-10-30 17:09:43.708225]  Unzipping Data... \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:10:58.197063]  Downloaded False to /content/input/abrehamatlaw0-spinoza-ds-datapreparer-simsim-cum-0-it-2-3 \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:10:58.197277]  Generating checksum for '/content/input/abrehamatlaw0-spinoza-ds-datapreparer-simsim-cum-0-it-2-3' \u001b[0m\n",
            "\u001b[94m PID:272 [2024-10-30 17:10:58.246152]  Checksum: 54b0f6ae5fdad5500a8fe11309fdbce8794e0ca278d3cc9bedce880c63d9e480 \u001b[0m\n"
          ]
        }
      ],
      "source": [
        "DATA_ROOT = \"/content/input\"\n",
        "DATASETS = [\n",
        "    f\"abrehamatlaw0/spinoza-ds-datapreparer-simsim-cum-0-it-2-{i}\"\n",
        "    for i in range(0,4)\n",
        "]\n",
        "KERNEL_MODE = False\n",
        "ZIP_FILENAME = \"out.zip\"\n",
        "download_data(DATA_ROOT, DATASETS, ZIP_FILENAME, kernel_mode=KERNEL_MODE)\n",
        "CONTAINERS = [os.path.join(DATA_ROOT, container) for container in os.listdir(DATA_ROOT)]\n",
        "DATA_PATHES, TEST_DATA_PATHES = [\n",
        "    [\n",
        "        os.path.join(container, \"out\", type_)\n",
        "        for container in CONTAINERS\n",
        "    ]\n",
        "    for type_ in [\"train\", \"test\"]\n",
        "]\n",
        "\n",
        "NOTEBOOK_ID = \"abrehamalemu/rtrader-training-exp-0-linear-108-cum-0-it-4-tot\"\n",
        "MODEL_ID = NOTEBOOK_ID.replace(\"/\", \"-\")\n",
        "\n",
        "NUM_FILES = None\n",
        "DATA_CACHE_SIZE = 2\n",
        "DATALOADER_WORKERS = 4\n",
        "\n",
        "VOCAB_SIZE = 431\n",
        "DROPOUT = 0.3\n",
        "LAYER_SIZES = [4096 for _ in range(8)] + [VOCAB_SIZE + 1]\n",
        "HIDDEN_ACTIVATION = nn.LeakyReLU()\n",
        "INIT_FUNCTION = None\n",
        "NORM = [True] + [False for _ in LAYER_SIZES[1:]]\n",
        "BLOCK_SIZE = 1148\n",
        "LR = 1e-5\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "TIMEOUT = int(10*60*60)\n",
        "\n",
        "DTYPE = torch.float32\n",
        "NP_DTYPE = np.float32\n",
        "\n",
        "MODEL_URL = None\n",
        "SAVE_PATH = os.path.abspath(os.path.join(\"./out\", f\"{MODEL_ID}.zip\"))\n",
        "\n",
        "METRIC_REPOSITORY = MongoDBMetricRepository(\n",
        "    Config.MONGODB_URL,\n",
        "    MODEL_ID\n",
        ")\n",
        "\n",
        "CALLBACKS = [\n",
        "    StoreCheckpointCallback(path=SAVE_PATH),\n",
        "    MetricCallback(\n",
        "       METRIC_REPOSITORY\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhSeYq_VnU0_"
      },
      "outputs": [],
      "source": [
        "repository = CheckpointRepository(\n",
        "    ServiceProvider.provide_file_storage()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18Atba5forzG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7efba9e7-981e-43dc-e194-901fb53d34ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/r_trader/lib/utils/torch_utils/model_handler.py:96: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict_lazy(torch.load(os.path.join(dirname, 'model_state.pth'), map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[+]Using loaded model...\n"
          ]
        }
      ],
      "source": [
        "state_model = repository.get(MODEL_ID)\n",
        "# state_model = None\n",
        "if state_model is None:\n",
        "    print(\"[+]Creating a new model...\")\n",
        "    if USE_FF:\n",
        "        ff = LinearModel(\n",
        "            dropout_rate=FF_DROPOUT,\n",
        "            layer_sizes=FF_LINEAR_LAYERS,\n",
        "            hidden_activation=FF_LINEAR_ACTIVATION,\n",
        "            init_fn=FF_LINEAR_INIT,\n",
        "            norm=FF_LINEAR_NORM\n",
        "        )\n",
        "    else:\n",
        "        ff = None\n",
        "\n",
        "    indicators = Indicators(\n",
        "        delta=INDICATORS_DELTA,\n",
        "        so=INDICATORS_SO,\n",
        "        rsi=INDICATORS_RSI\n",
        "    )\n",
        "\n",
        "    model = CNN(\n",
        "        extra_len=EXTRA_LEN,\n",
        "        conv_channels=CHANNELS,\n",
        "        kernel_sizes=KERNEL_SIZES,\n",
        "        hidden_activation=ACTIVATION,\n",
        "        pool_sizes=POOL_SIZES,\n",
        "        dropout_rate=DROPOUT_RATE,\n",
        "        padding=PADDING,\n",
        "        avg_pool=AVG_POOL,\n",
        "        linear_collapse=LINEAR_COLLAPSE,\n",
        "        norm=NORM,\n",
        "        ff_block=ff,\n",
        "        indicators=indicators,\n",
        "        input_size=BLOCK_SIZE\n",
        "    )\n",
        "\n",
        "else:\n",
        "    print(\"[+]Using loaded model...\")\n",
        "    state, model = state_model\n",
        "state = TrainingState(\n",
        "    epoch=0,\n",
        "    batch=0,\n",
        "    id=MODEL_ID\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_zgSaPho5Iw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d440de5a-7662-4d1b-ca98-51a1c340215c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "dataset = BaseDataset(\n",
        "    root_dirs=DATA_PATHES,\n",
        "    out_dtypes=NP_DTYPE,\n",
        "    num_files=NUM_FILES\n",
        ")\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=DATALOADER_WORKERS, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTCo4ngapg_w"
      },
      "outputs": [],
      "source": [
        "test_dataset = BaseDataset(\n",
        "    root_dirs=TEST_DATA_PATHES,\n",
        "    out_dtypes=NP_DTYPE,\n",
        "    num_files=NUM_FILES\n",
        ")\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=DATALOADER_WORKERS, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v4Y7Z-3phoF"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model, callbacks=CALLBACKS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AWSlZiSpu4n"
      },
      "outputs": [],
      "source": [
        "trainer.cls_loss_function = nn.CrossEntropyLoss()\n",
        "trainer.reg_loss_function = nn.MSELoss()\n",
        "trainer.optimizer = Adam(trainer.model.parameters(), lr=LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxxTng07pwFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df8e462-e13d-447b-93cc-efd672f0b1c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "class TimeoutException(Exception):\n",
        "    pass\n",
        "\n",
        "def handle_timeout(*args, **kwargs):\n",
        "    raise TimeoutException()\n",
        "\n",
        "signal.signal(signal.SIGALRM, handle_timeout)\n",
        "signal.alarm(TIMEOUT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUMBtB_gpxMO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3641c14-f9d5-4ee7-e55d-eab9aa797a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Summary\n",
            "Layer Name\t\t\t\t\t\t\tNumber of Parameters\n",
            "====================================================================================================\n",
            "layers.0.weight\t\t\t4702208\n",
            "layers.0.bias\t\t\t4096\n",
            "layers.1.weight\t\t\t16777216\n",
            "layers.1.bias\t\t\t4096\n",
            "layers.2.weight\t\t\t16777216\n",
            "layers.2.bias\t\t\t4096\n",
            "layers.3.weight\t\t\t16777216\n",
            "layers.3.bias\t\t\t4096\n",
            "layers.4.weight\t\t\t16777216\n",
            "layers.4.bias\t\t\t4096\n",
            "layers.5.weight\t\t\t16777216\n",
            "layers.5.bias\t\t\t4096\n",
            "layers.6.weight\t\t\t16777216\n",
            "layers.6.bias\t\t\t4096\n",
            "layers.7.weight\t\t\t16777216\n",
            "layers.7.bias\t\t\t4096\n",
            "layers.8.weight\t\t\t1769472\n",
            "layers.8.bias\t\t\t432\n",
            "norms.0.weight\t\t\t1148\n",
            "norms.0.bias\t\t\t1148\n",
            "====================================================================================================\n",
            "Total Params:123947688\n",
            "[+]Shuffling dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/32310 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1 loss: 3.8694212436676025(cls: 3.869420051574707, reg: 1.0757888730950071e-06): 100%|██████████| 32310/32310 [31:35<00:00, 17.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed, loss: 3.869572639465332(cls: 3.8695716857910156, reg: 1.0756822348412243e-06)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: loss: 3.8303513526916504(cls: 3.830350160598755, reg: 1.4399951169252745e-06)\n",
            "[+]Uploading /content/out/abrehamalemu-rtrader-training-exp-0-linear-108-cum-0-it-4-tot.zip => /Apps/RTrader\n",
            "[+]Shuffling dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 loss: 280.632568359375(cls: 29.704322814941406, reg: 250.91664123535156): 100%|██████████| 32310/32310 [31:19<00:00, 17.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 completed, loss: 280.5555419921875(cls: 29.697221755981445, reg: 250.8467559814453)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: loss: 3.825141191482544(cls: 3.825132131576538, reg: 8.132395123539027e-06)\n",
            "[+]Uploading /content/out/abrehamalemu-rtrader-training-exp-0-linear-108-cum-0-it-4-tot.zip => /Apps/RTrader\n",
            "[+]Shuffling dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 loss: 3.884852647781372(cls: 3.8848483562469482, reg: 4.069478563906159e-06): 100%|██████████| 32310/32310 [31:23<00:00, 17.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 completed, loss: 3.884812355041504(cls: 3.88480806350708, reg: 4.0688969420443755e-06)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: loss: 3.8166067600250244(cls: 3.816606044769287, reg: 1.3407229744188953e-06)\n",
            "[+]Uploading /content/out/abrehamalemu-rtrader-training-exp-0-linear-108-cum-0-it-4-tot.zip => /Apps/RTrader\n",
            "[+]Shuffling dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 loss: 3.8590447902679443(cls: 3.859043598175049, reg: 1.0942170547423302e-06):  63%|██████▎   | 20335/32310 [19:47<11:41, 17.08it/s]"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    trainer.train(dataloader, val_dataloader=test_dataloader, epochs=EPOCHS, progress=True, progress_interval=100, state=state, cls_loss_only=False)\n",
        "except TimeoutException:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Te77z0npzDI"
      },
      "outputs": [],
      "source": [
        "ModelHandler.save(model, SAVE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeTbBfkbp0gK"
      },
      "outputs": [],
      "source": [
        "repository.update(trainer.state, trainer.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvmKQtpNp2JR"
      },
      "outputs": [],
      "source": [
        "metrics = MetricsContainer()\n",
        "for metric in METRIC_REPOSITORY.get_all():\n",
        "    metrics.add_metric(metric)\n",
        "\n",
        "for i in range(3):\n",
        "    train_losses = [metric.value[i] for metric in metrics.filter_metrics(source=0)]\n",
        "    val_losses = [metric.value[i] for metric in metrics.filter_metrics(source=1)]\n",
        "    plt.figure()\n",
        "    plt.plot(train_losses)\n",
        "    plt.plot(val_losses)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bR0mDMR1p5Wc"
      },
      "outputs": [],
      "source": [
        "for X, y in test_dataloader:\n",
        "    break\n",
        "y_hat = model(X.to(trainer.device)).detach().cpu().numpy()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))\n",
        "    softmax_x = exp_x / np.sum(exp_x)\n",
        "    return softmax_x\n",
        "\n",
        "def scale(x):\n",
        "    x = softmax(x)\n",
        "    x = x / np.max(x)\n",
        "    return x\n",
        "\n",
        "for i in range(y_hat.shape[0]):\n",
        "    plt.figure()\n",
        "    plt.plot(y[i, :-1])\n",
        "    plt.plot(scale(y_hat[i, :-1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRMZT2L8p6z3"
      },
      "outputs": [],
      "source": [
        "!rm -fr /content/r_trader"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i8hEMv4rKeJf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}